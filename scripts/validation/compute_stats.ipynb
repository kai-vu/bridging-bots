{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a52498f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "from itertools import combinations\n",
    "from scipy.stats import shapiro, probplot, ttest_rel, wilcoxon, kruskal, mannwhitneyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d137e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../output/summary-statistics/merged_summary_statistics.csv')\n",
    "metrics = ['Class_Compliance', 'Property_Compliance', 'Class_Coverage', 'Property_Coverage']\n",
    "models = df['model'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b132e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['llava-llama3', 'llama4-scout', 'llama4-maverick', 'gpt-o1',\n",
       "       'gpt-4.1-nano'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c97cf17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: scipy.stats.shapiro: Input data has range zero. The results may not be accurate.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    }
   ],
   "source": [
    "normality_results = []\n",
    "grouped = df.groupby(['model', 'method'])\n",
    "\n",
    "for (model, method), group in grouped:\n",
    "    cov_diff = group['Class_Coverage'] - group['Property_Coverage']\n",
    "    com_diff = group['Class_Compliance'] - group['Property_Compliance']\n",
    "    if len(cov_diff) < 3 or len(com_diff) < 3:\n",
    "        continue\n",
    "    cov_stat, cov_p = shapiro(cov_diff)\n",
    "    com_stat, com_p = shapiro(com_diff)\n",
    "    normality_results.append({\n",
    "        'model': model,\n",
    "        'method': method,\n",
    "        'n': len(group),\n",
    "        'Coverage_p': cov_p,\n",
    "        'Coverage_normal': cov_p > 0.05,\n",
    "        'Compliance_p': com_p,\n",
    "        'Compliance_normal': com_p > 0.05\n",
    "    })\n",
    "normality_df = pd.DataFrame(normality_results)\n",
    "#print(normality_df.sort_values(by=['model', 'method']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2abf4fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              model               metric  observation_median  action_median  \\\n",
      "10  llama4-maverick       Class_Coverage                0.50       0.833333   \n",
      "11  llama4-maverick    Property_Coverage                1.00       0.875000   \n",
      "13           gpt-o1  Property_Compliance                0.80       0.875000   \n",
      "14           gpt-o1       Class_Coverage                0.75       0.833333   \n",
      "\n",
      "     p_value  significant  \n",
      "10  0.000014         True  \n",
      "11  0.046159         True  \n",
      "13  0.003401         True  \n",
      "14  0.013291         True  \n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for model in models:\n",
    "    df_model = df[df['model'] == model]\n",
    "    \n",
    "    for metric in metrics:\n",
    "        obs_values = df_model[df_model['graph_type'] == 'observation'][metric].dropna()\n",
    "        act_values = df_model[df_model['graph_type'] == 'action'][metric].dropna()\n",
    "\n",
    "        if len(obs_values) > 1 and len(act_values) > 1:\n",
    "            stat, p = mannwhitneyu(obs_values, act_values, alternative='two-sided')\n",
    "            results.append({\n",
    "                'model': model,\n",
    "                'metric': metric,\n",
    "                'observation_median': obs_values.median(),\n",
    "                'action_median': act_values.median(),\n",
    "                'p_value': p,\n",
    "                'significant': p < 0.05\n",
    "            })\n",
    "\n",
    "modelwise_test_df = pd.DataFrame(results)\n",
    "print(modelwise_test_df[modelwise_test_df['significant'] == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e65ad3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 model_method_1           model_method_2          metric  \\\n",
      "54          llava-llama3 | i2kg            gpt-o1 | i2kg  Avg_Compliance   \n",
      "222       llama4-maverick | dpe            gpt-o1 | i2kg  Avg_Compliance   \n",
      "138          llama4-scout | dpe            gpt-o1 | i2kg  Avg_Compliance   \n",
      "57          llava-llama3 | i2kg            gpt-o1 | d2kg    Avg_Coverage   \n",
      "293               gpt-o1 | d2kg  gpt-4.1-nano | d2kg-rag    Avg_Coverage   \n",
      "..                          ...                      ...             ...   \n",
      "87          llava-llama3 | d2kg            gpt-o1 | d2kg    Avg_Coverage   \n",
      "266  llama4-maverick | d2kg-rag            gpt-o1 | d2kg  Avg_Compliance   \n",
      "170         llama4-scout | i2kg      gpt-4.1-nano | d2kg  Avg_Compliance   \n",
      "267  llama4-maverick | d2kg-rag            gpt-o1 | d2kg    Avg_Coverage   \n",
      "240      llama4-maverick | i2kg            gpt-o1 | d2kg  Avg_Compliance   \n",
      "\n",
      "          p_value  significant  median_1  median_2  \n",
      "54   5.276898e-09         True  0.000000  0.908333  \n",
      "222  5.276898e-09         True  0.000000  0.908333  \n",
      "138  5.276898e-09         True  0.000000  0.908333  \n",
      "57   5.276898e-09         True  0.000000  0.854167  \n",
      "293  5.276898e-09         True  0.854167  0.000000  \n",
      "..            ...          ...       ...       ...  \n",
      "87   3.001690e-02         True  0.864583  0.854167  \n",
      "266  3.029407e-02         True  0.888889  0.927083  \n",
      "170  3.426562e-02         True  0.763889  0.200000  \n",
      "267  4.145377e-02         True  0.875000  0.854167  \n",
      "240  4.222798e-02         True  0.894444  0.927083  \n",
      "\n",
      "[239 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Add average metrics\n",
    "df['Avg_Compliance'] = (df['Class_Compliance'] + df['Property_Compliance']) / 2\n",
    "df['Avg_Coverage'] = (df['Class_Coverage'] + df['Property_Coverage']) / 2\n",
    "\n",
    "# Get unique (model, method) pairs\n",
    "model_method_pairs = df[['model', 'method']].drop_duplicates().values.tolist()\n",
    "\n",
    "# Compare each pair\n",
    "results = []\n",
    "\n",
    "for (model1, method1), (model2, method2) in combinations(model_method_pairs, 2):\n",
    "    group1 = df[(df['model'] == model1) & (df['method'] == method1)]\n",
    "    group2 = df[(df['model'] == model2) & (df['method'] == method2)]\n",
    "    \n",
    "    for metric in ['Avg_Compliance', 'Avg_Coverage']:\n",
    "        values1 = group1[metric].dropna()\n",
    "        values2 = group2[metric].dropna()\n",
    "        \n",
    "        # Ensure there's enough data to compare\n",
    "        if len(values1) > 1 and len(values2) > 1:\n",
    "            stat, p = mannwhitneyu(values1, values2, alternative='two-sided')\n",
    "            results.append({\n",
    "                'model_method_1': f'{model1} | {method1}',\n",
    "                'model_method_2': f'{model2} | {method2}',\n",
    "                'metric': metric,\n",
    "                'p_value': p,\n",
    "                'significant': p < 0.05,\n",
    "                'median_1': values1.median(),\n",
    "                'median_2': values2.median()\n",
    "            })\n",
    "\n",
    "# Create a results DataFrame\n",
    "diff_results_df = pd.DataFrame(results)\n",
    "\n",
    "# Show significant differences\n",
    "significant_results = diff_results_df[diff_results_df['significant'] == True]\n",
    "print(significant_results.sort_values(by='p_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54624e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Full_Parse_OK'] = df['Full_Parse_OK'].astype(bool)\n",
    "aggregation_functions = {\n",
    "    'Full_Parse_OK': 'sum'\n",
    "}\n",
    "\n",
    "float_columns = df.select_dtypes(include=['float64', 'int64']).columns.difference(['run'])\n",
    "for col in float_columns:\n",
    "    aggregation_functions[col] = 'mean'\n",
    "\n",
    "aggregated_df = df.groupby(['model', 'graph_type', 'method']).agg(aggregation_functions).reset_index()\n",
    "aggregated_df = aggregated_df.rename(columns={'Full_Parse_OK': 'Full_Parse_OK_Count'})\n",
    "aggregated_df.to_csv('../../output/summary-statistics/aggregated_summary_statistics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f892a657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb2ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
